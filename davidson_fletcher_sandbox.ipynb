{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    We loop over words in a dataset, and for each word, we look at a context window around the word. \n",
    "    \n",
    "    We generate pairs of (pivot_word, other_word_from_same_context) with label 1,\n",
    "    and pairs of (pivot_word, random_word) with label 0 (skip-gram method).\n",
    "    \n",
    "    We use the layer WordContextProduct to learn embeddings for the word couples,\n",
    "    and compute a proximity score between the embeddings (= p(context|word)),\n",
    "    trained with our positive and negative labels.\n",
    "    \n",
    "    We then use the weights computed by WordContextProduct to encode words \n",
    "    and demonstrate that the geometry of the embedding space \n",
    "    captures certain useful semantic properties.\n",
    "    \n",
    "    Read more about skip-gram in this particularly gnomic paper by Mikolov et al.: \n",
    "        http://arxiv.org/pdf/1301.3781v3.pdf\n",
    "        \n",
    "    Note: you should run this on GPU, otherwise training will be quite slow. \n",
    "    On a EC2 GPU instance, expect 3 hours per 10e6 comments (~10e8 words) per epoch with dim_proj=256.\n",
    "    Should be much faster on a modern GPU.\n",
    "    \n",
    "    GPU command:\n",
    "        THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python skipgram_word_embeddings.py\n",
    "        \n",
    "    Dataset: 5,845,908 Hacker News comments. \n",
    "    Obtain the dataset at: \n",
    "        https://mega.co.nz/#F!YohlwD7R!wec0yNO86SeaNGIYQBOR0A \n",
    "        (HNCommentsAll.1perline.json.bz2)\n",
    "'''\n",
    "\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "#import theano\n",
    "import six.moves.cPickle\n",
    "import os, re, json\n",
    "\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.optimizers import SGD, RMSprop, Adagrad\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from keras.models import Sequential\n",
    "import keras.layers.embeddings import WordContextProduct, Embedding\n",
    "from six.moves import range\n",
    "from six.moves import zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000 # vocabulary size: top 50,000 most common words in data\n",
    "skip_top = 100 # ignore top 100 most common words\n",
    "nb_epoch = 1\n",
    "dim_proj = 256 # embedding space dimension\n",
    "\n",
    "save = True\n",
    "load_model = False\n",
    "load_tokenizer = False\n",
    "train_model = True\n",
    "save_dir = os.path.expanduser(\"~/.keras/models\")\n",
    "model_load_fname = \"HN_skipgram_model.pkl\"\n",
    "model_save_fname = \"HN_skipgram_model.pkl\"\n",
    "tokenizer_fname = \"HN_tokenizer.pkl\"\n",
    "\n",
    "#data_path = os.path.expanduser(\"~/\")+\"HNCommentsAll.1perline.json\"\n",
    "#data_path = \"fotr.txt\"\n",
    "data_path = \"data/short_data/\"\n",
    "\n",
    "# text preprocessing utils\n",
    "html_tags = re.compile(r'<.*?>')\n",
    "to_replace = [('&#x27;', \"'\")]\n",
    "hex_tags = re.compile(r'&.*?;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(comment):\n",
    "    c = str(comment.encode(\"utf-8\"))\n",
    "    c = html_tags.sub(' ', c)\n",
    "    for tag, char in to_replace:\n",
    "        c = c.replace(tag, char)\n",
    "    c = hex_tags.sub(' ', c)\n",
    "    return c\n",
    "\n",
    "def text_generator(path=data_path):\n",
    "    f = open(path)\n",
    "    for i, l in enumerate(f):\n",
    "        comment_data = json.loads(l)\n",
    "        comment_text = comment_data[\"comment_text\"]\n",
    "        comment_text = clean_comment(comment_text)\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        yield comment_text\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model management\n",
    "if load_tokenizer:\n",
    "    print('Load tokenizer...')\n",
    "    tokenizer = six.moves.cPickle.load(open(os.path.join(save_dir, tokenizer_fname), 'rb'))\n",
    "else:\n",
    "    print(\"Fit tokenizer...\")\n",
    "    tokenizer = text.Tokenizer(nb_words=max_features)\n",
    "    tokenizer.fit_on_texts(text_generator())\n",
    "    if save:\n",
    "        print(\"Save tokenizer...\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        six.moves.cPickle.dump(tokenizer, open(os.path.join(save_dir, tokenizer_fname), \"wb\"))\n",
    "\n",
    "# training process\n",
    "if train_model:\n",
    "    if load_model:\n",
    "        print('Load model...')\n",
    "        model = six.moves.cPickle.load(open(os.path.join(save_dir, model_load_fname), 'rb'))\n",
    "    else:\n",
    "        print('Build model...')\n",
    "        model = Sequential()\n",
    "        model.add(embeddings.WordContextProduct(max_features, proj_dim=dim_proj, init=\"uniform\"))\n",
    "        model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "    sampling_table = sequence.make_sampling_table(max_features)\n",
    "\n",
    "    for e in range(nb_epoch):\n",
    "        print('-'*40)\n",
    "        print('Epoch', e)\n",
    "        print('-'*40)\n",
    "\n",
    "        progbar = generic_utils.Progbar(tokenizer.document_count)\n",
    "        samples_seen = 0\n",
    "        losses = []\n",
    "        \n",
    "        for i, seq in enumerate(tokenizer.texts_to_sequences_generator(text_generator())):\n",
    "            # get skipgram couples for one text in the dataset\n",
    "            couples, labels = sequence.skipgrams(seq, max_features, window_size=4, negative_samples=1., sampling_table=sampling_table)\n",
    "            if couples:\n",
    "                # one gradient update per sentence (one sentence = a few 1000s of word couples)\n",
    "                X = np.array(couples, dtype=\"int32\")\n",
    "                loss = model.train(X, labels)\n",
    "                losses.append(loss)\n",
    "                if len(losses) % 100 == 0:\n",
    "                    progbar.update(i, values=[(\"loss\", np.mean(losses))])\n",
    "                    losses = []\n",
    "                samples_seen += len(labels)\n",
    "        print('Samples seen:', samples_seen)\n",
    "    print(\"Training completed!\")\n",
    "\n",
    "    if save:\n",
    "        print(\"Saving model...\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        six.moves.cPickle.dump(model, open(os.path.join(save_dir, model_save_fname), \"wb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"It's test time!\")\n",
    "\n",
    "# recover the embedding weights trained with skipgram:\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "# we no longer need this\n",
    "del model\n",
    "\n",
    "weights[:skip_top] = np.zeros((skip_top, dim_proj))\n",
    "norm_weights = np_utils.normalize(weights)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(v, k) for k, v in list(word_index.items())])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "def embed_word(w):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return None\n",
    "    return norm_weights[i]\n",
    "\n",
    "def closest_to_point(point, nb_closest=10):\n",
    "    proximities = np.dot(norm_weights, point)\n",
    "    tups = list(zip(list(range(len(proximities))), proximities))\n",
    "    tups.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [(reverse_word_index.get(t[0]), t[1]) for t in tups[:nb_closest]]  \n",
    "\n",
    "def closest_to_word(w, nb_closest=10):\n",
    "    i = word_index.get(w)\n",
    "    if (not i) or (i<skip_top) or (i>=max_features):\n",
    "        return []\n",
    "    return closest_to_point(norm_weights[i].T, nb_closest)\n",
    "\n",
    "\n",
    "''' the resuls in comments below were for: \n",
    "    5.8M HN comments\n",
    "    dim_proj = 256\n",
    "    nb_epoch = 2\n",
    "    optimizer = rmsprop\n",
    "    loss = mse\n",
    "    max_features = 50000\n",
    "    skip_top = 100\n",
    "    negative_samples = 1.\n",
    "    window_size = 4\n",
    "    and frequency subsampling of factor 10e-5. \n",
    "'''\n",
    "\n",
    "words = [\"article\", # post, story, hn, read, comments\n",
    "\"3\", # 6, 4, 5, 2\n",
    "\"two\", # three, few, several, each\n",
    "\"great\", # love, nice, working, looking\n",
    "\"data\", # information, memory, database\n",
    "\"money\", # company, pay, customers, spend\n",
    "\"years\", # ago, year, months, hours, week, days\n",
    "\"android\", # ios, release, os, mobile, beta\n",
    "\"javascript\", # js, css, compiler, library, jquery, ruby\n",
    "\"look\", # looks, looking\n",
    "\"business\", # industry, professional, customers\n",
    "\"company\", # companies, startup, founders, startups\n",
    "\"after\", # before, once, until\n",
    "\"own\", # personal, our, having\n",
    "\"us\", # united, country, american, tech, diversity, usa, china, sv\n",
    "\"using\", # javascript, js, tools (lol)\n",
    "\"here\", # hn, post, comments\n",
    "]\n",
    "\n",
    "for w in words:\n",
    "    res = closest_to_word(w)\n",
    "    print('====', w)\n",
    "    for r in res:\n",
    "        print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
